{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "traindata = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\traindataz.mat')#traindata = scio.loadmat('E:\\\\00心电\\\\af\\\\traindata.mat')#E:\\00心电\\af\n",
    "data = traindata['traindataz']\n",
    "trainlable = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\trainlable')#trainlable = scio.loadmat('E:\\\\00心电\\\\af\\\\trainlable.mat') \n",
    "lable = trainlable['trainlable'][0]\n",
    "#traindata = scio.loadmat('E:\\\\00心电\\\\af\\\\traindata.mat')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\2018挑战赛10s\\\\testdata201810s.mat')#traindata = scio.loadmat('E:\\\\00心电\\\\af\\\\traindata.mat')#E:\\00心电\\af\n",
    "testdata = testdata['testdata201810s'][:,0:7]\n",
    "testlable = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\2018挑战赛10s\\\\testtable201810s.mat')#trainlable = scio.loadmat('E:\\\\00心电\\\\af\\\\trainlable.mat') \n",
    "testlable = testlable['testtable201810s'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.shuffle(data) \n",
    "np.random.seed(1)\n",
    "np.random.shuffle(lable)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(testdata) \n",
    "np.random.seed(1)\n",
    "np.random.shuffle(testlable)\n",
    "\n",
    "import torch\n",
    "input = torch.FloatTensor(data)\n",
    "label = torch.LongTensor(lable)\n",
    "input_test = torch.FloatTensor(testdata)\n",
    "label_test = torch.LongTensor(testlable)\n",
    "\n",
    "input_train = input[0:60001]\n",
    "label_train = label[0:60001]\n",
    "\n",
    "input_valid = input[60001:]\n",
    "label_valid = label[60001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as Fun\n",
    "# 定义BP神经网络\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = Fun.relu(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lay = [7,1024,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7828, grad_fn=<NllLossBackward>) 0.49704171597140046 0.4987679373822293 0.5\n",
      "1 tensor(0.6800, grad_fn=<NllLossBackward>) 0.5000416659722338 0.49963762864183214 0.6080931263858093\n",
      "2 tensor(0.6092, grad_fn=<NllLossBackward>) 0.5093415109748171 0.5064502101753877 0.8292682926829268\n",
      "3 tensor(0.5302, grad_fn=<NllLossBackward>) 0.844085931901135 0.8411363965792144 0.9478935698447893\n",
      "4 tensor(0.4656, grad_fn=<NllLossBackward>) 0.9247679205346577 0.9202783012030729 0.9279379157427938\n",
      "5 tensor(0.4195, grad_fn=<NllLossBackward>) 0.924551257479042 0.9228873749818814 0.8957871396895787\n",
      "6 tensor(0.3809, grad_fn=<NllLossBackward>) 0.9221846302561624 0.92245252935208 0.9029933481152993\n",
      "7 tensor(0.3444, grad_fn=<NllLossBackward>) 0.9259345677572041 0.9246267575010871 0.9279379157427938\n",
      "8 tensor(0.3129, grad_fn=<NllLossBackward>) 0.9293511774803753 0.9273807798231628 0.9523281596452328\n",
      "9 tensor(0.2891, grad_fn=<NllLossBackward>) 0.9288845185913568 0.9249166545876214 0.9628603104212861\n",
      "10 tensor(0.2716, grad_fn=<NllLossBackward>) 0.9277178713688106 0.9236121176982172 0.9667405764966741\n",
      "11 tensor(0.2576, grad_fn=<NllLossBackward>) 0.9269178847019216 0.9231772720684157 0.967849223946785\n",
      "12 tensor(0.2453, grad_fn=<NllLossBackward>) 0.9274178763687272 0.92346716915495 0.9684035476718403\n",
      "13 tensor(0.2343, grad_fn=<NllLossBackward>) 0.9284345260912318 0.9243368604145529 0.9684035476718403\n",
      "14 tensor(0.2252, grad_fn=<NllLossBackward>) 0.9293178447025883 0.9249166545876214 0.9689578713968958\n",
      "15 tensor(0.2184, grad_fn=<NllLossBackward>) 0.930401159980667 0.9262211914770256 0.967849223946785\n",
      "16 tensor(0.2135, grad_fn=<NllLossBackward>) 0.9305178247029217 0.9278156254529641 0.967849223946785\n",
      "17 tensor(0.2097, grad_fn=<NllLossBackward>) 0.9303011616473059 0.928685316712567 0.967849223946785\n",
      "18 tensor(0.2065, grad_fn=<NllLossBackward>) 0.9304678255362411 0.9282504710827656 0.9684035476718403\n",
      "19 tensor(0.2036, grad_fn=<NllLossBackward>) 0.9304678255362411 0.9291201623423685 0.9684035476718403\n",
      "20 tensor(0.2009, grad_fn=<NllLossBackward>) 0.930884485258579 0.9278156254529641 0.9689578713968958\n",
      "21 tensor(0.1987, grad_fn=<NllLossBackward>) 0.9311344810919818 0.9275257283664299 0.970620842572062\n",
      "22 tensor(0.1970, grad_fn=<NllLossBackward>) 0.9308678188696855 0.9263661400202928 0.9722838137472284\n",
      "23 tensor(0.1959, grad_fn=<NllLossBackward>) 0.9305678238696021 0.9262211914770256 0.9733924611973392\n",
      "24 tensor(0.1952, grad_fn=<NllLossBackward>) 0.9307011549807503 0.9265110885635599 0.9750554323725056\n",
      "25 tensor(0.1946, grad_fn=<NllLossBackward>) 0.9304678255362411 0.9260762429337586 0.9745011086474501\n",
      "26 tensor(0.1939, grad_fn=<NllLossBackward>) 0.9305844902584957 0.9262211914770256 0.9745011086474501\n",
      "27 tensor(0.1932, grad_fn=<NllLossBackward>) 0.9307844869252179 0.9269459341933614 0.9745011086474501\n",
      "28 tensor(0.1926, grad_fn=<NllLossBackward>) 0.9311844802586624 0.9266560371068271 0.9745011086474501\n",
      "29 tensor(0.1921, grad_fn=<NllLossBackward>) 0.9314178097031716 0.9265110885635599 0.975609756097561\n",
      "30 tensor(0.1918, grad_fn=<NllLossBackward>) 0.9314511424809586 0.9270908827366285 0.975609756097561\n",
      "31 tensor(0.1915, grad_fn=<NllLossBackward>) 0.9316678055365744 0.9265110885635599 0.9750554323725056\n",
      "32 tensor(0.1913, grad_fn=<NllLossBackward>) 0.9316844719254679 0.9275257283664299 0.9745011086474501\n",
      "33 tensor(0.1911, grad_fn=<NllLossBackward>) 0.9318011366477226 0.9282504710827656 0.9739467849223947\n",
      "34 tensor(0.1908, grad_fn=<NllLossBackward>) 0.931851135814403 0.9281055225394985 0.9739467849223947\n",
      "35 tensor(0.1905, grad_fn=<NllLossBackward>) 0.9320677988700188 0.9281055225394985 0.9761640798226164\n",
      "36 tensor(0.1902, grad_fn=<NllLossBackward>) 0.9321011316478058 0.9282504710827656 0.9767184035476718\n",
      "37 tensor(0.1899, grad_fn=<NllLossBackward>) 0.9321844635922735 0.9281055225394985 0.9767184035476718\n",
      "38 tensor(0.1896, grad_fn=<NllLossBackward>) 0.9321511308144864 0.9272358312798956 0.9761640798226164\n",
      "39 tensor(0.1894, grad_fn=<NllLossBackward>) 0.9321844635922735 0.9275257283664299 0.9745011086474501\n",
      "40 tensor(0.1891, grad_fn=<NllLossBackward>) 0.932234462758954 0.9278156254529641 0.9722838137472284\n",
      "41 tensor(0.1888, grad_fn=<NllLossBackward>) 0.9324177930367827 0.9285403681692999 0.970620842572062\n",
      "42 tensor(0.1885, grad_fn=<NllLossBackward>) 0.9324177930367827 0.928685316712567 0.9700665188470067\n",
      "43 tensor(0.1882, grad_fn=<NllLossBackward>) 0.9324677922034633 0.928685316712567 0.9695121951219512\n",
      "44 tensor(0.1878, grad_fn=<NllLossBackward>) 0.9327677872035466 0.9292651108856356 0.9684035476718403\n",
      "45 tensor(0.1875, grad_fn=<NllLossBackward>) 0.9330344494258429 0.929699956515437 0.9656319290465631\n",
      "46 tensor(0.1872, grad_fn=<NllLossBackward>) 0.9332011133147781 0.9302797506885055 0.9639689578713969\n",
      "47 tensor(0.1869, grad_fn=<NllLossBackward>) 0.9332177797036716 0.9308595448615742 0.9628603104212861\n",
      "48 tensor(0.1865, grad_fn=<NllLossBackward>) 0.9332677788703522 0.9308595448615742 0.958980044345898\n",
      "49 tensor(0.1862, grad_fn=<NllLossBackward>) 0.9334011099815003 0.9312943904913755 0.95509977827051\n",
      "50 tensor(0.1858, grad_fn=<NllLossBackward>) 0.9334011099815003 0.9312943904913755 0.9528824833702882\n",
      "51 tensor(0.1855, grad_fn=<NllLossBackward>) 0.9333844435926067 0.9315842875779099 0.9506651884700665\n",
      "52 tensor(0.1852, grad_fn=<NllLossBackward>) 0.9334511091481809 0.9318741846644442 0.9478935698447893\n",
      "53 tensor(0.1848, grad_fn=<NllLossBackward>) 0.9336011066482225 0.9320191332077112 0.9456762749445676\n",
      "54 tensor(0.1845, grad_fn=<NllLossBackward>) 0.9337844369260513 0.9321640817509784 0.9423503325942351\n",
      "55 tensor(0.1842, grad_fn=<NllLossBackward>) 0.9338511024816253 0.9318741846644442 0.9406873614190687\n",
      "56 tensor(0.1838, grad_fn=<NllLossBackward>) 0.9338511024816253 0.9320191332077112 0.9395787139689579\n",
      "57 tensor(0.1835, grad_fn=<NllLossBackward>) 0.9339844335927735 0.9323090302942455 0.938470066518847\n",
      "58 tensor(0.1831, grad_fn=<NllLossBackward>) 0.9342344294261762 0.9325989273807799 0.9379157427937915\n",
      "59 tensor(0.1828, grad_fn=<NllLossBackward>) 0.9343344277595373 0.932743875924047 0.938470066518847\n",
      "60 tensor(0.1824, grad_fn=<NllLossBackward>) 0.9342844285928568 0.932743875924047 0.9379157427937915\n",
      "61 tensor(0.1821, grad_fn=<NllLossBackward>) 0.9343510941484309 0.9328888244673141 0.9373614190687362\n",
      "62 tensor(0.1817, grad_fn=<NllLossBackward>) 0.9346010899818337 0.9328888244673141 0.9356984478935698\n",
      "63 tensor(0.1814, grad_fn=<NllLossBackward>) 0.9346677555374077 0.9330337730105812 0.9340354767184036\n",
      "64 tensor(0.1811, grad_fn=<NllLossBackward>) 0.9347510874818753 0.9333236700971155 0.9323725055432373\n",
      "65 tensor(0.1807, grad_fn=<NllLossBackward>) 0.9349010849819169 0.9334686186403827 0.9323725055432373\n",
      "66 tensor(0.1804, grad_fn=<NllLossBackward>) 0.9351344144264262 0.9336135671836499 0.9323725055432373\n",
      "67 tensor(0.1800, grad_fn=<NllLossBackward>) 0.9353010783153614 0.9336135671836499 0.9323725055432373\n",
      "68 tensor(0.1797, grad_fn=<NllLossBackward>) 0.9354010766487225 0.9336135671836499 0.9351441241685144\n",
      "69 tensor(0.1794, grad_fn=<NllLossBackward>) 0.9355010749820837 0.9337585157269169 0.9362527716186253\n",
      "70 tensor(0.1790, grad_fn=<NllLossBackward>) 0.9356010733154447 0.9339034642701841 0.9395787139689579\n",
      "71 tensor(0.1787, grad_fn=<NllLossBackward>) 0.9357010716488059 0.9344832584432526 0.9406873614190687\n",
      "72 tensor(0.1784, grad_fn=<NllLossBackward>) 0.9359010683155281 0.9347731555297869 0.9417960088691796\n",
      "73 tensor(0.1781, grad_fn=<NllLossBackward>) 0.9359844002599956 0.9347731555297869 0.9429046563192904\n",
      "74 tensor(0.1777, grad_fn=<NllLossBackward>) 0.9360510658155697 0.9347731555297869 0.9462305986696231\n",
      "75 tensor(0.1774, grad_fn=<NllLossBackward>) 0.9361177313711438 0.9347731555297869 0.9484478935698448\n",
      "76 tensor(0.1771, grad_fn=<NllLossBackward>) 0.9361677305378243 0.9349181040730541 0.9506651884700665\n",
      "77 tensor(0.1768, grad_fn=<NllLossBackward>) 0.936251062482292 0.9350630526163212 0.9523281596452328\n",
      "78 tensor(0.1765, grad_fn=<NllLossBackward>) 0.9364343927601206 0.9350630526163212 0.9545454545454546\n",
      "79 tensor(0.1762, grad_fn=<NllLossBackward>) 0.9365177247045883 0.9353529497028555 0.9562084257206208\n",
      "80 tensor(0.1759, grad_fn=<NllLossBackward>) 0.9366010566490558 0.9356428467893898 0.9600886917960089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 tensor(0.1756, grad_fn=<NllLossBackward>) 0.9366843885935234 0.9359327438759241 0.9611973392461197\n",
      "82 tensor(0.1754, grad_fn=<NllLossBackward>) 0.9369343844269262 0.9362226409624583 0.9634146341463414\n",
      "83 tensor(0.1751, grad_fn=<NllLossBackward>) 0.9370510491491808 0.9363675895057255 0.9656319290465631\n",
      "84 tensor(0.1748, grad_fn=<NllLossBackward>) 0.9371177147047549 0.9365125380489926 0.9667405764966741\n",
      "85 tensor(0.1745, grad_fn=<NllLossBackward>) 0.9372343794270096 0.9368024351355269 0.9667405764966741\n",
      "86 tensor(0.1743, grad_fn=<NllLossBackward>) 0.937251045815903 0.9366574865922598 0.9684035476718403\n",
      "87 tensor(0.1740, grad_fn=<NllLossBackward>) 0.9373343777603707 0.9368024351355269 0.9684035476718403\n",
      "88 tensor(0.1737, grad_fn=<NllLossBackward>) 0.9374677088715188 0.936947383678794 0.9684035476718403\n",
      "89 tensor(0.1735, grad_fn=<NllLossBackward>) 0.9375843735937734 0.936947383678794 0.9700665188470067\n",
      "90 tensor(0.1732, grad_fn=<NllLossBackward>) 0.9378343694271762 0.9370923322220611 0.9722838137472284\n",
      "91 tensor(0.1729, grad_fn=<NllLossBackward>) 0.9380176997050049 0.9373822293085955 0.9722838137472284\n",
      "92 tensor(0.1727, grad_fn=<NllLossBackward>) 0.938117698038366 0.9378170749383968 0.9722838137472284\n",
      "93 tensor(0.1724, grad_fn=<NllLossBackward>) 0.9382176963717271 0.9381069720249311 0.9728381374722838\n",
      "94 tensor(0.1722, grad_fn=<NllLossBackward>) 0.9383176947050883 0.9381069720249311 0.9722838137472284\n",
      "95 tensor(0.1719, grad_fn=<NllLossBackward>) 0.9384676922051299 0.9383968691114655 0.9739467849223947\n",
      "96 tensor(0.1716, grad_fn=<NllLossBackward>) 0.938534357760704 0.9382519205681983 0.9745011086474501\n",
      "97 tensor(0.1714, grad_fn=<NllLossBackward>) 0.9386676888718521 0.9383968691114655 0.9750554323725056\n",
      "98 tensor(0.1711, grad_fn=<NllLossBackward>) 0.9388010199830003 0.9385418176547325 0.975609756097561\n",
      "99 tensor(0.1709, grad_fn=<NllLossBackward>) 0.9388843519274679 0.938976663284534 0.975609756097561\n",
      "100 tensor(0.1707, grad_fn=<NllLossBackward>) 0.9389176847052549 0.938976663284534 0.975609756097561\n",
      "101 tensor(0.1704, grad_fn=<NllLossBackward>) 0.939017683038616 0.9388317147412668 0.975609756097561\n",
      "102 tensor(0.1702, grad_fn=<NllLossBackward>) 0.9392010133164447 0.938976663284534 0.9745011086474501\n",
      "103 tensor(0.1699, grad_fn=<NllLossBackward>) 0.9394510091498475 0.938976663284534 0.9739467849223947\n",
      "104 tensor(0.1697, grad_fn=<NllLossBackward>) 0.9395843402609957 0.938976663284534 0.9739467849223947\n",
      "105 tensor(0.1695, grad_fn=<NllLossBackward>) 0.9396343394276762 0.938976663284534 0.9739467849223947\n",
      "106 tensor(0.1693, grad_fn=<NllLossBackward>) 0.9396843385943567 0.9394115089143354 0.9739467849223947\n",
      "107 tensor(0.1690, grad_fn=<NllLossBackward>) 0.939817669705505 0.9394115089143354 0.9739467849223947\n",
      "108 tensor(0.1688, grad_fn=<NllLossBackward>) 0.939884335261079 0.9394115089143354 0.9750554323725056\n",
      "109 tensor(0.1686, grad_fn=<NllLossBackward>) 0.9399843335944401 0.9394115089143354 0.975609756097561\n",
      "110 tensor(0.1684, grad_fn=<NllLossBackward>) 0.9401009983166947 0.9395564574576025 0.9750554323725056\n",
      "111 tensor(0.1682, grad_fn=<NllLossBackward>) 0.9401676638722688 0.9397014060008697 0.9750554323725056\n",
      "112 tensor(0.1680, grad_fn=<NllLossBackward>) 0.9402509958167364 0.9397014060008697 0.9750554323725056\n",
      "113 tensor(0.1677, grad_fn=<NllLossBackward>) 0.9404343260945651 0.9402812001739382 0.9750554323725056\n",
      "114 tensor(0.1675, grad_fn=<NllLossBackward>) 0.9406509891501809 0.9402812001739382 0.975609756097561\n",
      "115 tensor(0.1673, grad_fn=<NllLossBackward>) 0.9407009883168613 0.9405710972604725 0.975609756097561\n",
      "116 tensor(0.1671, grad_fn=<NllLossBackward>) 0.940817653039116 0.9407160458037397 0.975609756097561\n",
      "117 tensor(0.1669, grad_fn=<NllLossBackward>) 0.9409176513724771 0.941005942890274 0.975609756097561\n",
      "118 tensor(0.1667, grad_fn=<NllLossBackward>) 0.9410843152614123 0.9411508914335411 0.975609756097561\n",
      "119 tensor(0.1665, grad_fn=<NllLossBackward>) 0.9411676472058799 0.9411508914335411 0.975609756097561\n",
      "120 tensor(0.1662, grad_fn=<NllLossBackward>) 0.9412843119281346 0.9411508914335411 0.975609756097561\n",
      "121 tensor(0.1660, grad_fn=<NllLossBackward>) 0.9413343110948151 0.9411508914335411 0.975609756097561\n",
      "122 tensor(0.1658, grad_fn=<NllLossBackward>) 0.9413176447059216 0.941005942890274 0.975609756097561\n",
      "123 tensor(0.1656, grad_fn=<NllLossBackward>) 0.9414843085948568 0.9411508914335411 0.975609756097561\n",
      "124 tensor(0.1654, grad_fn=<NllLossBackward>) 0.9416676388726855 0.9415857370633425 0.975609756097561\n",
      "125 tensor(0.1652, grad_fn=<NllLossBackward>) 0.941717638039366 0.9417306856066097 0.975609756097561\n",
      "126 tensor(0.1650, grad_fn=<NllLossBackward>) 0.94178430359494 0.9417306856066097 0.975609756097561\n",
      "127 tensor(0.1648, grad_fn=<NllLossBackward>) 0.9419009683171947 0.9417306856066097 0.975609756097561\n",
      "128 tensor(0.1646, grad_fn=<NllLossBackward>) 0.9420176330394493 0.9414407885200754 0.9761640798226164\n",
      "129 tensor(0.1644, grad_fn=<NllLossBackward>) 0.942134297761704 0.9415857370633425 0.9761640798226164\n",
      "130 tensor(0.1642, grad_fn=<NllLossBackward>) 0.9421842969283846 0.9417306856066097 0.9761640798226164\n",
      "131 tensor(0.1640, grad_fn=<NllLossBackward>) 0.9423176280395327 0.9417306856066097 0.9761640798226164\n",
      "132 tensor(0.1638, grad_fn=<NllLossBackward>) 0.9423842935951068 0.9415857370633425 0.9761640798226164\n",
      "133 tensor(0.1636, grad_fn=<NllLossBackward>) 0.9424176263728938 0.9415857370633425 0.9761640798226164\n",
      "134 tensor(0.1634, grad_fn=<NllLossBackward>) 0.9425176247062549 0.9420205826931439 0.9761640798226164\n",
      "135 tensor(0.1632, grad_fn=<NllLossBackward>) 0.9426009566507225 0.9423104797796782 0.9761640798226164\n",
      "136 tensor(0.1630, grad_fn=<NllLossBackward>) 0.94268428859519 0.9423104797796782 0.9761640798226164\n",
      "137 tensor(0.1628, grad_fn=<NllLossBackward>) 0.9427842869285512 0.9424554283229454 0.9767184035476718\n",
      "138 tensor(0.1626, grad_fn=<NllLossBackward>) 0.9428009533174447 0.9426003768662125 0.9767184035476718\n",
      "139 tensor(0.1624, grad_fn=<NllLossBackward>) 0.9428842852619123 0.9426003768662125 0.9767184035476718\n",
      "140 tensor(0.1622, grad_fn=<NllLossBackward>) 0.9429176180396993 0.9426003768662125 0.9767184035476718\n",
      "141 tensor(0.1620, grad_fn=<NllLossBackward>) 0.9429842835952734 0.9428902739527468 0.9767184035476718\n",
      "142 tensor(0.1618, grad_fn=<NllLossBackward>) 0.943100948317528 0.9428902739527468 0.9761640798226164\n",
      "143 tensor(0.1617, grad_fn=<NllLossBackward>) 0.9431509474842086 0.9430352224960139 0.9761640798226164\n",
      "144 tensor(0.1615, grad_fn=<NllLossBackward>) 0.9432342794286762 0.9428902739527468 0.9761640798226164\n",
      "145 tensor(0.1613, grad_fn=<NllLossBackward>) 0.9432842785953568 0.9428902739527468 0.9761640798226164\n",
      "146 tensor(0.1611, grad_fn=<NllLossBackward>) 0.9433176113731437 0.9427453254094796 0.9761640798226164\n",
      "147 tensor(0.1609, grad_fn=<NllLossBackward>) 0.943484275262079 0.9427453254094796 0.9767184035476718\n",
      "148 tensor(0.1607, grad_fn=<NllLossBackward>) 0.943517608039866 0.9424554283229454 0.9761640798226164\n",
      "149 tensor(0.1605, grad_fn=<NllLossBackward>) 0.9435676072065465 0.9426003768662125 0.9761640798226164\n",
      "150 tensor(0.1603, grad_fn=<NllLossBackward>) 0.9437342710954817 0.9427453254094796 0.9761640798226164\n",
      "151 tensor(0.1601, grad_fn=<NllLossBackward>) 0.9438342694288429 0.9430352224960139 0.975609756097561\n",
      "152 tensor(0.1600, grad_fn=<NllLossBackward>) 0.9438676022066299 0.9431801710392811 0.975609756097561\n",
      "153 tensor(0.1598, grad_fn=<NllLossBackward>) 0.9438842685955234 0.9433251195825482 0.975609756097561\n",
      "154 tensor(0.1596, grad_fn=<NllLossBackward>) 0.944000933317778 0.9434700681258154 0.975609756097561\n",
      "155 tensor(0.1594, grad_fn=<NllLossBackward>) 0.9441842635956067 0.9437599652123496 0.975609756097561\n",
      "156 tensor(0.1592, grad_fn=<NllLossBackward>) 0.9441342644289262 0.9440498622988839 0.9761640798226164\n",
      "157 tensor(0.1590, grad_fn=<NllLossBackward>) 0.9441842635956067 0.9441948108421511 0.9761640798226164\n",
      "158 tensor(0.1588, grad_fn=<NllLossBackward>) 0.9442009299845002 0.9443397593854181 0.9750554323725056\n",
      "159 tensor(0.1587, grad_fn=<NllLossBackward>) 0.9442009299845002 0.9446296564719524 0.9745011086474501\n",
      "160 tensor(0.1585, grad_fn=<NllLossBackward>) 0.9442342627622873 0.9446296564719524 0.9739467849223947\n",
      "161 tensor(0.1583, grad_fn=<NllLossBackward>) 0.9443175947067549 0.9446296564719524 0.9745011086474501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 tensor(0.1581, grad_fn=<NllLossBackward>) 0.944450925817903 0.9447746050152196 0.9739467849223947\n",
      "163 tensor(0.1579, grad_fn=<NllLossBackward>) 0.9445509241512642 0.9447746050152196 0.9733924611973392\n",
      "164 tensor(0.1577, grad_fn=<NllLossBackward>) 0.9446509224846252 0.9450645021017539 0.9733924611973392\n",
      "165 tensor(0.1576, grad_fn=<NllLossBackward>) 0.9447509208179864 0.9450645021017539 0.9733924611973392\n",
      "166 tensor(0.1574, grad_fn=<NllLossBackward>) 0.9447509208179864 0.945209450645021 0.9733924611973392\n",
      "167 tensor(0.1572, grad_fn=<NllLossBackward>) 0.9448175863735604 0.9454993477315553 0.9733924611973392\n",
      "168 tensor(0.1571, grad_fn=<NllLossBackward>) 0.9449675838736021 0.9456442962748225 0.9711751662971175\n",
      "169 tensor(0.1569, grad_fn=<NllLossBackward>) 0.9451175813736438 0.9457892448180896 0.970620842572062\n",
      "170 tensor(0.1567, grad_fn=<NllLossBackward>) 0.9451009149847502 0.9459341933613568 0.9711751662971175\n",
      "171 tensor(0.1566, grad_fn=<NllLossBackward>) 0.9451009149847502 0.9460791419046238 0.9717294900221729\n",
      "172 tensor(0.1564, grad_fn=<NllLossBackward>) 0.9451175813736438 0.9459341933613568 0.9717294900221729\n",
      "173 tensor(0.1563, grad_fn=<NllLossBackward>) 0.9452342460958985 0.9459341933613568 0.9717294900221729\n",
      "174 tensor(0.1561, grad_fn=<NllLossBackward>) 0.9452675788736854 0.9459341933613568 0.970620842572062\n",
      "175 tensor(0.1560, grad_fn=<NllLossBackward>) 0.945317578040366 0.9460791419046238 0.970620842572062\n",
      "176 tensor(0.1558, grad_fn=<NllLossBackward>) 0.9454009099848336 0.9459341933613568 0.970620842572062\n",
      "177 tensor(0.1557, grad_fn=<NllLossBackward>) 0.9455175747070882 0.9459341933613568 0.9711751662971175\n",
      "178 tensor(0.1555, grad_fn=<NllLossBackward>) 0.9455009083181947 0.9457892448180896 0.9700665188470067\n",
      "179 tensor(0.1554, grad_fn=<NllLossBackward>) 0.9454175763737271 0.9457892448180896 0.9700665188470067\n",
      "180 tensor(0.1552, grad_fn=<NllLossBackward>) 0.9454342427626207 0.9457892448180896 0.9700665188470067\n",
      "181 tensor(0.1551, grad_fn=<NllLossBackward>) 0.9455342410959817 0.9459341933613568 0.9700665188470067\n",
      "182 tensor(0.1549, grad_fn=<NllLossBackward>) 0.9455342410959817 0.9459341933613568 0.970620842572062\n",
      "183 tensor(0.1548, grad_fn=<NllLossBackward>) 0.9456175730404494 0.9460791419046238 0.970620842572062\n",
      "184 tensor(0.1546, grad_fn=<NllLossBackward>) 0.9456509058182364 0.9460791419046238 0.9700665188470067\n",
      "185 tensor(0.1545, grad_fn=<NllLossBackward>) 0.9457009049849169 0.946224090447891 0.970620842572062\n",
      "186 tensor(0.1543, grad_fn=<NllLossBackward>) 0.9457509041515975 0.9463690389911581 0.970620842572062\n",
      "187 tensor(0.1542, grad_fn=<NllLossBackward>) 0.9458175697071716 0.9465139875344253 0.9700665188470067\n",
      "188 tensor(0.1540, grad_fn=<NllLossBackward>) 0.9458509024849586 0.9468038846209595 0.9700665188470067\n",
      "189 tensor(0.1539, grad_fn=<NllLossBackward>) 0.9459175680405326 0.9470937817074938 0.9700665188470067\n",
      "190 tensor(0.1538, grad_fn=<NllLossBackward>) 0.9459509008183197 0.947238730250761 0.970620842572062\n",
      "191 tensor(0.1536, grad_fn=<NllLossBackward>) 0.9459842335961067 0.947238730250761 0.9700665188470067\n",
      "192 tensor(0.1535, grad_fn=<NllLossBackward>) 0.9460675655405744 0.947238730250761 0.9700665188470067\n",
      "193 tensor(0.1533, grad_fn=<NllLossBackward>) 0.9460675655405744 0.9473836787940281 0.9700665188470067\n",
      "194 tensor(0.1532, grad_fn=<NllLossBackward>) 0.9460342327627873 0.9473836787940281 0.9700665188470067\n",
      "195 tensor(0.1531, grad_fn=<NllLossBackward>) 0.9459842335961067 0.947238730250761 0.970620842572062\n",
      "196 tensor(0.1529, grad_fn=<NllLossBackward>) 0.9460675655405744 0.947238730250761 0.9695121951219512\n",
      "197 tensor(0.1528, grad_fn=<NllLossBackward>) 0.9461508974850419 0.9473836787940281 0.9689578713968958\n",
      "198 tensor(0.1527, grad_fn=<NllLossBackward>) 0.946250895818403 0.9473836787940281 0.967849223946785\n",
      "199 tensor(0.1525, grad_fn=<NllLossBackward>) 0.9463342277628706 0.947238730250761 0.9667405764966741\n"
     ]
    }
   ],
   "source": [
    "#思路是训练，然后脚本\n",
    "net = Net(n_feature=Lay[0], n_hidden=Lay[1],n_output=Lay[2])\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "# SGD:随机梯度下降法\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "# 设定损失函数\n",
    "\n",
    "\n",
    "#开始训练\n",
    "#并在每100轮输出结果（这里最好每轮都记录我们的损失和精度，需要改进代码）\n",
    "losslist = []\n",
    "speficlist = []\n",
    "spefictestlist = []\n",
    "spefictrainlist = []\n",
    "for i in range(200):\n",
    "    out = net(input_train)\n",
    "    loss = loss_func(out, label_train)\n",
    "    if(i%1 == 0):\n",
    "        #print(loss)\n",
    "        losslist.append(loss)\n",
    "        out_test = net(input_valid)\n",
    "        # out是一个计算矩阵\n",
    "        prediction = torch.max(out_test, 1)[1]\n",
    "        pred_y = prediction.numpy()\n",
    "        # 预测y输出数列\n",
    "        target_y = label_valid.data.numpy()\n",
    "        speficvalid = np.sum(pred_y == target_y)/(np.sum(pred_y == target_y) +np.sum(pred_y != target_y) )\n",
    "        speficlist.append(speficvalid)\n",
    "        #plt.figure('Draw')\n",
    "\n",
    "        out_train = net(input_train)\n",
    "        # out是一个计算矩阵\n",
    "        prediction = torch.max(out_train, 1)[1]\n",
    "        pred_y = prediction.numpy()\n",
    "        # 预测y输出数列\n",
    "        target_y = label_train.data.numpy()\n",
    "        spefictrain = np.sum(pred_y == target_y)/(np.sum(pred_y == target_y) +np.sum(pred_y != target_y) )\n",
    "        spefictrainlist.append(spefictrain)\n",
    "        \n",
    "        out_test = net(input_test)\n",
    "        # out是一个计算矩阵\n",
    "        prediction = torch.max(out_test, 1)[1]\n",
    "        pred_y = prediction.numpy()\n",
    "        # 预测y输出数列\n",
    "        target_y = label_test.data.numpy()\n",
    "        spefic = np.sum(pred_y == target_y)/(np.sum(pred_y == target_y) +np.sum(pred_y != target_y) )\n",
    "        spefictestlist.append(spefic)\n",
    "        \n",
    "        print(i,loss,spefictrain,speficvalid,spefic)\n",
    "        #plt.plot(speficlist)  # plot绘制折线图\n",
    "        #plt.draw()  # 显示绘图\n",
    "        #plt.pause(2)  #显示5秒\n",
    "        #plt.close()\n",
    "    # 输出与label对比\n",
    "#     if(i > 150 and i%10 == 0):\n",
    "#         torch.save(net.state_dict(), '3_layer7-8-2_'+str(i)+'.pt')\n",
    "    optimizer.zero_grad()\n",
    "    # 初始化\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_front= open(\"AF_classifier_front_7_x_2.cpp\")  \n",
    "#逐行写入\n",
    "for line in cpp_front.readlines():  \n",
    "    with open(\"AF_classifier_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\",\"a\") as cpp:\n",
    "        cpp.write(line)  \n",
    "cpp_front.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0651,  0.3904,  0.1375,  ...,  0.2665, -0.0235, -0.3006],\n",
      "        [ 0.3089,  0.1239,  0.1991,  ..., -0.0222,  0.0841,  0.1126],\n",
      "        [-0.2340,  0.0752,  0.3128,  ..., -0.1739, -0.0416, -0.1253],\n",
      "        ...,\n",
      "        [ 0.1609,  0.1990, -0.2910,  ..., -0.1702,  0.4801, -0.0928],\n",
      "        [-0.2084, -0.0612, -0.3301,  ...,  0.2490,  0.0828, -0.3014],\n",
      "        [ 0.3197,  0.1642,  0.3659,  ..., -0.2284, -0.1556,  0.0070]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0651,  0.3904,  0.1375,  ...,  0.2665, -0.0235, -0.3006],\n",
      "        [ 0.3089,  0.1239,  0.1991,  ..., -0.0222,  0.0841,  0.1126],\n",
      "        [-0.2340,  0.0752,  0.3128,  ..., -0.1739, -0.0416, -0.1253],\n",
      "        ...,\n",
      "        [ 0.1609,  0.1990, -0.2910,  ..., -0.1702,  0.4801, -0.0928],\n",
      "        [-0.2084, -0.0612, -0.3301,  ...,  0.2490,  0.0828, -0.3014],\n",
      "        [ 0.3197,  0.1642,  0.3659,  ..., -0.2284, -0.1556,  0.0070]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3161, -0.2577, -0.2716,  ..., -0.2656,  0.0406, -0.0479],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3161, -0.2577, -0.2716,  ..., -0.2656,  0.0406, -0.0479],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.9631e-03, -2.0849e-03,  2.4274e-02,  ..., -2.5450e-01,\n",
      "         -1.8325e-02, -2.3864e-04],\n",
      "        [ 2.2720e-02,  5.1405e-02,  1.9058e-03,  ...,  2.5809e-01,\n",
      "         -1.5607e-02,  2.2806e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.9631e-03, -2.0849e-03,  2.4274e-02,  ..., -2.5450e-01,\n",
      "         -1.8325e-02, -2.3864e-04],\n",
      "        [ 2.2720e-02,  5.1405e-02,  1.9058e-03,  ...,  2.5809e-01,\n",
      "         -1.5607e-02,  2.2806e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.5486e-03,  3.3478e-05], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.5486e-03,  3.3478e-05], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#将模型参数转换为C/C++中的数组写入到文本中\n",
    "fp = open(\"AF_classifier_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\",'a')\n",
    "flag = True\n",
    "k=1\n",
    "for name,param in net.named_parameters():  \n",
    "\n",
    "    print(param)\n",
    "    temp =  param\n",
    "    temp = temp.float()\n",
    "    print(temp)\n",
    "    temp1 = temp.numpy\n",
    "    temp2= temp.tolist()\n",
    "    if flag == True:\n",
    "        flag = False\n",
    "        fp.write('ap_f Hw'+str(k)+'['+str(len(temp2))+']['+str(len(temp2[0]))+'] = {')\n",
    "        for i in range(len(temp2)-1):\n",
    "            fp.write('{')\n",
    "            for j in range(len(temp2[0])-1):\n",
    "                fp.write(str(temp2[i][j])+',')\n",
    "            fp.write(str(temp2[i][len(temp2[0])-1])+'},\\n')\n",
    "        #尾巴\n",
    "        fp.write('{')\n",
    "        for j in range(len(temp2[0])-1):\n",
    "            fp.write(str(temp2[len(temp2)-1][j])+',')\n",
    "        fp.write(str(temp2[len(temp2)-1][len(temp2[0])-1])+'}')\n",
    "        fp.write('};\\n\\n\\n')\n",
    "    else:\n",
    "        flag = True\n",
    "        fp.write('ap_f Hb'+str(k)+'['+str(len(temp2))+'] = {')\n",
    "        k +=1\n",
    "        for i in range(len(temp2)-1):\n",
    "            fp.write(str(temp2[i])+',')\n",
    "        fp.write(str(temp2[len(temp2)-1]))\n",
    "        fp.write('};\\n\\n\\n')\n",
    "        \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap_f B[1024];\n",
      "ap_f C[2];\n"
     ]
    }
   ],
   "source": [
    "#将参数对应的程序转换为C/C++中的数组写入到文本中\n",
    "\n",
    "fp = open(\"AF_classifier_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\",'a')\n",
    "for i in range(1,len(Lay)):\n",
    "    print(\"ap_f \"+chr(ord('A')+i)+'['+str(Lay[i])+'];')\n",
    "    fp.write(\"ap_f \"+chr(ord('A')+i)+'['+str(Lay[i])+'];\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    for (int i = 0; i < 1024; i++) {\n",
      "        B[i] = 0;\n",
      "        for (int j = 0; j < 7; j++) {\n",
      "            B[i] = B[i] + A[j] * Hw1[i][j];\n",
      "        }\n",
      "        B[i] = B[i]/int_para + Hb1[i] ;\n",
      "        if(B[i] < 0){B[i] = 0;}\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "fp = open(\"AF_classifier_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\",'a')\n",
    "for i in range(len(Lay)-2):\n",
    "    print('    for (int i = 0; i < '+str(Lay[i+1])+'; i++) {')\n",
    "    print('        '+chr(ord('A')+i+1)+'[i] = 0;')\n",
    "    print('        for (int j = 0; j < '+str(Lay[i])+'; j++) {')\n",
    "    print('            '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i] + '+chr(ord('A')+i)+'[j] * Hw'+str(i+1)+'[i][j];')\n",
    "    print('        }')\n",
    "    print('        '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i]/int_para + Hb'+str(i+1)+'[i] ;')\n",
    "    print('        if('+chr(ord('A')+i+1)+'[i] < 0){'+chr(ord('A')+i+1)+'[i] = 0;}')\n",
    "    print('    }')\n",
    "    \n",
    "    fp.write('    for (int i = 0; i < '+str(Lay[i+1])+'; i++) {\\n')\n",
    "    fp.write('        '+chr(ord('A')+i+1)+'[i] = 0;\\n')\n",
    "    fp.write('        for (int j = 0; j < '+str(Lay[i])+'; j++) {\\n')\n",
    "    fp.write('            '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i] + '+chr(ord('A')+i)+'[j] * Hw'+str(i+1)+'[i][j];\\n')\n",
    "    fp.write('        }\\n')\n",
    "    fp.write('        '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i]/int_para + Hb'+str(i+1)+'[i] ;\\n')\n",
    "    fp.write('        if('+chr(ord('A')+i+1)+'[i] < 0){'+chr(ord('A')+i+1)+'[i] = 0;}\\n')\n",
    "    fp.write('    }\\n')   \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    for (int i = 0; i < 2; i++) {\n",
      "        C[i] = 0;\n",
      "        for (int j = 0; j < 1024; j++) {\n",
      "            C[i] = C[i] + B[j] * Hw2[i][j];\n",
      "        }\n",
      "        C[i] = C[i]/int_para + Hb2[i] ;\n",
      "    }\n",
      "    if (C[0] > C[1])\n",
      "        {result = 0 ;}\n",
      "    else\n",
      "        {result = 1 ;}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fp = open(\"AF_classifier_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\",'a')\n",
    "for i in range(len(Lay)-2,len(Lay)-1):\n",
    "    print('    for (int i = 0; i < '+str(Lay[i+1])+'; i++) {')\n",
    "    print('        '+chr(ord('A')+i+1)+'[i] = 0;')\n",
    "    print('        for (int j = 0; j < '+str(Lay[i])+'; j++) {')\n",
    "    print('            '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i] + '+chr(ord('A')+i)+'[j] * Hw'+str(i+1)+'[i][j];')\n",
    "    print('        }')\n",
    "    print('        '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i]/int_para + Hb'+str(i+1)+'[i] ;')\n",
    "    print('    }')\n",
    "    #开始写入result\n",
    "    print('    if ('+chr(ord('A')+i+1)+'[0] > '+chr(ord('A')+i+1)+'[1])')\n",
    "    print('        {result = 0 ;}')\n",
    "    print('    else')\n",
    "    print('        {result = 1 ;}')\n",
    "    print('}')\n",
    "    \n",
    "    fp.write('    for (int i = 0; i < '+str(Lay[i+1])+'; i++) {\\n')\n",
    "    fp.write('        '+chr(ord('A')+i+1)+'[i] = 0;\\n')\n",
    "    fp.write('        for (int j = 0; j < '+str(Lay[i])+'; j++) {\\n')\n",
    "    fp.write('            '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i] + '+chr(ord('A')+i)+'[j] * Hw'+str(i+1)+'[i][j];\\n')\n",
    "    fp.write('        }\\n')\n",
    "    fp.write('        '+chr(ord('A')+i+1)+'[i] = '+chr(ord('A')+i+1)+'[i]/int_para + Hb'+str(i+1)+'[i] ;\\n')\n",
    "    fp.write('    }\\n')\n",
    "    #开始写入result\n",
    "    fp.write('    if ('+chr(ord('A')+i+1)+'[0] > '+chr(ord('A')+i+1)+'[1])\\n')\n",
    "    fp.write('        {result = 0 ;}\\n')\n",
    "    fp.write('    else\\n')\n",
    "    fp.write('        {result = 1 ;}\\n')\n",
    "    fp.write('}\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "####写入tcl\n",
    "#写入tcl\n",
    "with open(\"script.tcl\",\"a\") as tcl:\n",
    "    tcl.write(\"\\n\\n\\n########################\\nopen_project HLS_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\"+\"\\n\")\n",
    "    tcl.write(\"set_top AF_classifier\\n\")\n",
    "    tcl.write(\"add_files \"+\"AF_classifier_\"+str(Lay[0])+\"_\"+str(Lay[1])+\"_\"+str(Lay[2])+\".cpp\\n\")\n",
    "\n",
    "tcl_end = open(\"script_end.tcl\")  \n",
    "for line in tcl_end.readlines():  \n",
    "    with open(\"script.tcl\",\"a\") as tcl:\n",
    "        tcl.write(line)  \n",
    "tcl_end.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
